{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice running the Permafrost Discovery Gateway visualization workflow\n",
    "\n",
    "- This notebook is designed to be run on the NCEAS Datateam server, because that is where the ice-wedge polygon data is archived.\n",
    "- Create a fresh Python environment and install the requirements with `pip install -r requirements.txt`\n",
    "- Execute staging, rasterization, and web-tiling steps.\n",
    "- Process 3 adjacent ice-wedge polygon files from [Wrangle Island](https://www.google.com/maps/place/Wrangel+Island/@71.3497058,179.8238705,9z/data=!4m6!3m5!1s0x50a70636a5f5033f:0xe1dca925085b4bc3!8m2!3d71.2488724!4d-179.9789208!16zL20vMDMyZnRq), off the coast of Russia\n",
    "- After running through these steps in chunks in this notebook, it's a great idea to transfer the code to a script and run as a `tmux` session. See `workflow.py` for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# visual checks & vector data wrangling\n",
    "import geopandas as gpd\n",
    "\n",
    "# staging\n",
    "import pdgstaging\n",
    "from pdgstaging import TileStager\n",
    "\n",
    "# rasterization & web-tiling\n",
    "import pdgraster\n",
    "from pdgraster import RasterTiler\n",
    "\n",
    "# logging\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pdgstaging import logging_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import 3 adjacent data files with ice-wedge polygon detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/jcohen/iwp_russia_subset_clipToFP_PR/'\n",
    "base_dir = Path(data_dir + 'iwp')\n",
    "filename = '*.shp'\n",
    "# To define each .shp file within each subdir as a string representation with forward slashes, use as_posix()\n",
    "# The ** represents that any subdir string can be present between the base_dir and the filename\n",
    "input = [p.as_posix() for p in base_dir.glob('**/' + filename)]\n",
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the footprint files for the ice-wedge polygon detections\n",
    "\n",
    "These files each represent the bounding box of the respective ice-wedge polygon detections file. This is used for deduplication where the scenes overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull filepaths for footprints in the same way we pulled IWP shp file paths\n",
    "base_dir_fp = Path(data_dir + 'footprints')\n",
    "# To define each .shp file within each subdir as a string representation with forward slashes, use as_posix()\n",
    "# The ** represents that any subdir string can be present between the base_dir and the filename\n",
    "fps = [p.as_posix() for p in base_dir_fp.glob('**/' + filename)]\n",
    "fps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on logging\n",
    "\n",
    "- Logging for the workflow is configured by the file `logging_config.py` within the `pdgstaging` package. We imported this configuration at the start of this notebook.\n",
    "- The written logging file `log.log` is written to the `/tmp` directory on Datateam. It's good practice to `mv` this file from `/tmp` to your working directory after each workflow run. In the script that runs this workflow, `workflow.py`, this command is added to the end so we don't forget. If the file is not moved, logging statements from the next run are simply appended.\n",
    "- When troubleshooting, it's helpful to ctrl + f for certain logged statements when troubleshooting. For example, if fewer files were staged than expected, you can search for \"error\" or \"failed\". If you are debugging a silent error and suspect that the issue has something to do with the order in which input files are processed, you can search for the input filenames to determine which was staged first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the files with their footprints\n",
    "\n",
    "Each of the following plots takes ~2-5 minutes to generate on Datateam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first pair: one file of IWP detections and the associated footprint file \n",
    "shp = gpd.read_file(input[0])\n",
    "footprint = gpd.read_file(fps[0])\n",
    "ax = shp.plot(color='none', edgecolor='green', linewidths=1.5, figsize=(14,14))\n",
    "footprint.plot(ax=ax, color='none', edgecolor='black', linewidths=0.5, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second pair\n",
    "shp = gpd.read_file(input[1])\n",
    "footprint = gpd.read_file(fps[1])\n",
    "ax = shp.plot(color='none', edgecolor='blue', linewidths=1.5, figsize=(14,14))\n",
    "footprint.plot(ax=ax, color='none', edgecolor='black', linewidths=0.5, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third pair\n",
    "shp = gpd.read_file(input[2])\n",
    "footprint = gpd.read_file(fps[2])\n",
    "ax = shp.plot(color='none', edgecolor='green', linewidths=1.5, figsize=(14,14))\n",
    "footprint.plot(ax=ax, color='none', edgecolor='black', linewidths=0.5, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2 non-coastal adjacent iwp files together, no footprints\n",
    "shp1 = gpd.read_file(input[1])\n",
    "shp2 = gpd.read_file(input[2])\n",
    "ax = shp1.plot(color='none', edgecolor='blue', linewidths=1.5, figsize=(14,14), alpha=0.3)\n",
    "shp2.plot(ax=ax, color='none', edgecolor='green', linewidths=0.5, figsize=(14,14), alpha=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the configuration and stager\n",
    "\n",
    "The configuration defines how the workflow is executed, such as if we deduplicate polygons, _how_ we deduplicate, the color palette, statistics that become bands in the output geoTIFFFs, etc. You can define the config in a number of ways:\n",
    "1. input the config directly to `TileStager()`\n",
    "2. _or_ define the config as an object in the script, and feed it into `TileStager` with `pdgstaging.TileStager(config)`, as we do in the next chunk. This is preferred over option 1 because we feed the same config into the `RasterTiler` later, and option 2 allows for the config to only be defined once.\n",
    "3. _or_ define the config within a separate `.py` or `.json` script and source it in with the following:\n",
    "\n",
    "    ```python\n",
    "    # config.py must be a script in same level of folder hierachy as this notebook \n",
    "    # that contains config defined as an object like `config = {...}` \n",
    "    import config \n",
    "    config = config.config\n",
    "    pdgstaging.TileStager(config)\n",
    "    ```\n",
    "\n",
    "#### Notes on configuration: \n",
    "- Many of the settings in the config specified below are defaults. You can find default values for the config [here](https://github.com/PermafrostDiscoveryGateway/viz-staging/blob/4f31e951600d54c128f76b48a47ec390261fb548/pdgstaging/ConfigManager.py#L351-L422).\n",
    "- The statistic(s) specified are calculated during rasterization, and each stat each becomes a band. You can add a separate stat, remove a stat, or change the details of them as desired. The \"name\" of each statistic can be anything, but the \"property\" needs to be either found within the vector data itself or be one of the custom stats defined in `pdgstaging`. Here, we use `area_per_pixel_area`, which is a custom one.\n",
    "- Color palette can be changed, but must be at least 2 values.\n",
    "- Input data types for the staging step should be vectors, like `.shp` or `.gpkg`. We specify `.shp` below because the IWP input data are shapefiles. \n",
    "\n",
    "For more information about the configuration, there is extensive documentation in [PermafrostDiscoveryGateway/viz-staging/pdgstaging/ConfigManager.py](https://github.com/PermafrostDiscoveryGateway/viz-staging/blob/main/pdgstaging/ConfigManager.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"deduplicate_clip_to_footprint\": True, \n",
    "  \"dir_input\": data_dir + \"iwp/\", \n",
    "  \"ext_input\": \".shp\",\n",
    "  \"ext_footprints\": \".shp\",\n",
    "  \"dir_footprints\": data_dir + \"footprints/\", \n",
    "  \"dir_staged\": \"staged/\",\n",
    "  \"dir_geotiff\": \"geotiff/\", \n",
    "  \"dir_web_tiles\": \"web_tiles/\", \n",
    "  \"filename_staging_summary\": \"staging_summary.csv\",\n",
    "  \"filename_rasterization_events\": \"raster_events.csv\",\n",
    "  \"filename_rasters_summary\": \"raster_summary.csv\",\n",
    "  \"filename_config\": \"config\",\n",
    "  \"simplify_tolerance\": 0.1,\n",
    "  \"tms_id\": \"WGS1984Quad\",\n",
    "  \"z_range\": [\n",
    "    0,\n",
    "    15\n",
    "  ],\n",
    "  \"geometricError\": 57,\n",
    "  \"z_coord\": 0,\n",
    "  \"statistics\": [\n",
    "    {\n",
    "      \"name\": \"iwp_coverage\",\n",
    "      \"weight_by\": \"area\",\n",
    "      \"property\": \"area_per_pixel_area\",\n",
    "      \"aggregation_method\": \"sum\",\n",
    "      \"resampling_method\": \"average\",\n",
    "      \"val_range\": [\n",
    "        0,\n",
    "        1\n",
    "      ],\n",
    "      \"palette\": [\n",
    "        \"#66339952\",\n",
    "        \"#ffcc00\"\n",
    "      ],\n",
    "      \"nodata_val\": 0,\n",
    "      \"nodata_color\": \"#ffffff00\"\n",
    "    },\n",
    "  ],\n",
    "  \"deduplicate_at\": [\n",
    "    \"raster\"\n",
    "  ],\n",
    "  \"deduplicate_keep_rules\": [\n",
    "    [\n",
    "      \"Date\",\n",
    "      \"larger\"\n",
    "    ]\n",
    "  ],\n",
    "  \"deduplicate_method\": \"footprints\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stager = TileStager(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staging\n",
    "\n",
    "The following chunk will create a new directory called `staged` and populate it with subdirectories and geopackage files. Each file is a tile in the Tile Matrix Set `WGS1984Quad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stager.stage_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of `staged` files produced: **3214**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasterization & web-tiling\n",
    "\n",
    "- Use the `pdgraster.RasterTiler()` function to create the `rasterizer`, then use it to execute the rasterize function: `rasterizer.rasterize_vectors()`.\n",
    "- Alternatively, can run these two steps as one with just `pdgraster.RasterTiler().rasterize_all()`. Just as in staging, you can input the config directly to `RasterTiler()`, or save the config as an object earlier in the script, or save the config as a separate script and source it in. \n",
    "- `rasterize_all()` is a wrapper for `rasterize_vectors()`. It pulls all staged filepaths from the staged dir and rasterizes all z-levels . We do _not_ use `rasterize_all()` when rasterizing in parallel with `parsl` or `ray`. It is exclusively used for small datasets that are not run in parallel. We have to create specific `@parsl` or `@ray.remote` wrapper functions around `stage()` and `rasterize_vectors()` if using those packages for staging, rasterizaiton, etc. For an example, see [here](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/8c1997a9d2456bcb79ba1b3ab0f82b3b2b30b141/IN_PROGRESS_VIZ_WORKFLOW.py#L668-L693) for how we rasterize in the ray workflow.\n",
    "- Rasterization when executed with `rasterize_all()` creates `.tif` files in the output `geotiff` dir, _and_ creates the same number of `.png` files in the output `web_tiles` dir.\n",
    "    - When we use `rasterize_vectors()` instead, we _only create the `.tif` files and not the `.png` files_. So that needs to be executed as a separate step with `rasterizer.webtiles_from_geotiffs()` after the `rasterize_vectors() step`, and _in between those steps_ we need to manually \"update the ranges\" in the rasterizer to ensure that the colors within one z-level of `.png` files looks appropriate when visualized in the portal. We will cross that bridge as necessay, just as we do [here](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/8c1997a9d2456bcb79ba1b3ab0f82b3b2b30b141/IN_PROGRESS_VIZ_WORKFLOW.py#L592) in the `ray` workflow on Delta.\n",
    "- The number of z-level 15 tiles in the `staged` dir should match the number of z-level 15 tiles in the `geotiff` and `web_tiles` dirs. The total number of files in both the `geotiff` and `web_tiles` dirs is a _lot_ more than the number of files in `staged` because `staged` _only contains the highest zoom level_ with no parent z-levels.\n",
    "- The web tiles are what we actually visualize on the PDG portal and local cesium. We create the rasters for summary stats (the data behind the web tiles, stored in bands of the `.tif` files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RasterTiler(config).rasterize_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of `geotiff` files produced (for all z-levels): **4376**\n",
    "\n",
    "Number of `web_tiles` produced (for all z-levels): **4376**\n",
    "  - The number is the same because we only calculated 1 statistic, so one band in each geoTIFF. Each raster band becomes one web tile, so if there are two statistics calculated, there will be twice as many web tiles as there are geoTIFFs.\n",
    "\n",
    "-----------\n",
    "\n",
    "You can use `rasterio` to plot the rasters if you want to get an idea what the statistics look like. Change the number to choose which band (statistic) to visualize. Example:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "\n",
    "file1 = rasterio.open('/path/to/raster.tif')\n",
    "\n",
    "plt.imshow(file1.read(1), cma = 'pink')\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the web tiles\n",
    "\n",
    "Ypu can simply open the `.png` files to view them one at a time, but it's much better to view them with Cesium! For steps for how to visualize the web tiles with local Cesium, see [documentation here in pdg-info](https://github.com/robyngit/pdg-info/blob/main/05_displaying-the-tiles.md#option-1-run-cesium-locally)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipToFP_PR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
