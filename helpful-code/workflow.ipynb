{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice running the Permafrost Discovery Gateway visualization workflow through web tiling\n",
    "\n",
    "- 3 adjacent ice wedge polygon files from [Wrangle Island](https://www.google.com/maps/place/Wrangel+Island/@71.3497058,179.8238705,9z/data=!4m6!3m5!1s0x50a70636a5f5033f:0xe1dca925085b4bc3!8m2!3d71.2488724!4d-179.9789208!16zL20vMDMyZnRq), off the coasts of Russia & Alaska\n",
    "- use environment with `viz-staging` and `viz-raster` installed\n",
    "- after running through these steps in chunks in this notebook, it's a great idea to transfer the code to a script and run as a `tmux` session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data import\n",
    "from pathlib import Path\n",
    "\n",
    "# staging\n",
    "import pdgstaging\n",
    "from pdgstaging import TileStager\n",
    "\n",
    "# rasterization\n",
    "import pdgraster\n",
    "from pdgraster import RasterTiler\n",
    "\n",
    "# 3D tiling\n",
    "from viz_3dtiles import TreeGenerator, BoundingVolumeRegion\n",
    "\n",
    "# visual checks\n",
    "import geopandas as gpd\n",
    "\n",
    "# logging\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import logging.handlers\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "In order to process 1 or 2 files instead of all 3 adjacent files on Wrangle Island, subset the `input` list of filepaths created below. Estimated times and number of files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('/home/jcohen/iwp_russia_subset_clipToFP_PR/iwp')\n",
    "filename = '*.shp'\n",
    "# To define each .shp file within each subdir as a string representation with forward slashes, use as_posix()\n",
    "# The ** represents that any subdir string can be present between the base_dir and the filename\n",
    "input = [p.as_posix() for p in base_dir.glob('**/' + filename)]\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull filepaths for footprints in the same way we pulled IWP shp file paths\n",
    "base_dir_fp = Path('/home/jcohen/iwp_russia_subset_clipToFP_PR/footprints')\n",
    "# To define each .shp file within each subdir as a string representation with forward slashes, use as_posix()\n",
    "# The ** represents that any subdir string can be present between the base_dir and the filename\n",
    "fps = [p.as_posix() for p in base_dir_fp.glob('**/' + filename)]\n",
    "fps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the logging configuration\n",
    "\n",
    "This prints logging statements to a file specified by the path in the config. Change the filepath as needed. There will be many logging statements written to that file. It's helpful to ctrl + f for certain logged statements when troubleshooting. For example, if fewer files were staged than expected, you can search for \"error\" or \"failed\". If you are debugging a silent error and suspect that the issue has something to do with the order in which input files are processed, you can search for the input filenames to determine which was staged first. In between runs, it's a good idea to delete the log from the past run, rename it, or archive it elsewhere so the next run's log does not append to the same log file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = logging.handlers.WatchedFileHandler(\n",
    "    os.environ.get(\"LOGFILE\", \"/home/jcohen/pdg-info/helpful-code/log.log\"))\n",
    "formatter = logging.Formatter(logging.BASIC_FORMAT)\n",
    "handler.setFormatter(formatter)\n",
    "root = logging.getLogger()\n",
    "root.setLevel(os.environ.get(\"LOGLEVEL\", \"INFO\"))\n",
    "root.addHandler(handler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the files with their footprints (optional)\n",
    "\n",
    "Each of the following plots takes ~2-5 minutes to generate. Size has been increased in order to see how the IWP polygons spill over the edge of the footprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first pair\n",
    "shp = gpd.read_file(input[0])\n",
    "footprint = gpd.read_file(fps[0])\n",
    "ax = shp.plot(color='none', edgecolor='green', linewidths=1.5, figsize=(14,14))\n",
    "footprint.plot(ax=ax, color='none', edgecolor='black', linewidths=0.5, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second pair\n",
    "shp = gpd.read_file(input[1])\n",
    "footprint = gpd.read_file(fps[1])\n",
    "ax = shp.plot(color='none', edgecolor='blue', linewidths=1.5, figsize=(14,14))\n",
    "footprint.plot(ax=ax, color='none', edgecolor='black', linewidths=0.5, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third pair\n",
    "shp = gpd.read_file(input[2])\n",
    "footprint = gpd.read_file(fps[2])\n",
    "ax = shp.plot(color='none', edgecolor='green', linewidths=1.5, figsize=(14,14))\n",
    "footprint.plot(ax=ax, color='none', edgecolor='black', linewidths=0.5, figsize=(14,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2 non-coastal adjacent iwp files together, no footprints\n",
    "shp1 = gpd.read_file(input[1])\n",
    "shp2 = gpd.read_file(input[2])\n",
    "ax = shp1.plot(color='none', edgecolor='blue', linewidths=1.5, figsize=(14,14), alpha=0.3)\n",
    "shp2.plot(ax=ax, color='none', edgecolor='green', linewidths=0.5, figsize=(14,14), alpha=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the stager\n",
    "\n",
    "You can input the config directly to `TileStager()`, or save the config as an object earlier in the script, or save the config as a separate `.py` or `.json` script and source it in with:\n",
    "```python\n",
    "# config.py must be a script in same level of folder hierachy as this notebook \n",
    "# that contains config defined as an object like `config = {...}` \n",
    "import config \n",
    "config = config.config\n",
    "pdgstaging.TileStager(config)\n",
    "```\n",
    "\n",
    "- Many of the settings in the config specified below are defaults. You can find default values for the config [here](https://github.com/PermafrostDiscoveryGateway/viz-staging/blob/4f31e951600d54c128f76b48a47ec390261fb548/pdgstaging/ConfigManager.py#L351-L422).\n",
    "- You'll notice that the statistics (calculated during rasterization, each becomes a band) are tailored towards ice wedge polygons. You can remove an entire statistic, or change them as desired. The \"name\" of each statistic can be anything.\n",
    "- I would not recommend changing the number of z-levels. \n",
    "- Color palette can be changed, but must be at least 2 values.\n",
    "- Input data types for the staging step should be vectors, like `.shp` or `.gpkg`. We specify `.shp` below because the IWP input data are shapefiles. \n",
    "\n",
    "For more information about the configuration, there is extensive documentation in [PermafrostDiscoveryGateway/viz-staging/pdgstaging/ConfigManager.py](https://github.com/PermafrostDiscoveryGateway/viz-staging/blob/main/pdgstaging/ConfigManager.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stager = TileStager({\n",
    "  \"deduplicate_clip_to_footprint\": True, \n",
    "  \"dir_input\": \"/home/jcohen/iwp_russia_subset_clipToFP_PR/iwp/\", \n",
    "  \"ext_input\": \".shp\",\n",
    "  \"ext_footprints\": \".shp\",\n",
    "  \"dir_footprints\": \"/home/jcohen/iwp_russia_subset_clipToFP_PR/footprints/\", \n",
    "  \"dir_staged\": \"staged/\",\n",
    "  \"dir_geotiff\": \"geotiff/\", \n",
    "  \"dir_web_tiles\": \"web_tiles/\", \n",
    "  \"filename_staging_summary\": \"staging_summary.csv\",\n",
    "  \"filename_rasterization_events\": \"raster_events.csv\",\n",
    "  \"filename_rasters_summary\": \"raster_summary.csv\",\n",
    "  \"filename_config\": \"config\",\n",
    "  \"simplify_tolerance\": 0.1,\n",
    "  \"tms_id\": \"WGS1984Quad\",\n",
    "  \"z_range\": [\n",
    "    0,\n",
    "    15\n",
    "  ],\n",
    "  \"geometricError\": 57,\n",
    "  \"z_coord\": 0,\n",
    "  \"statistics\": [\n",
    "    {\n",
    "      \"name\": \"iwp_coverage\",\n",
    "      \"weight_by\": \"area\",\n",
    "      \"property\": \"area_per_pixel_area\",\n",
    "      \"aggregation_method\": \"sum\",\n",
    "      \"resampling_method\": \"average\",\n",
    "      \"val_range\": [\n",
    "        0,\n",
    "        1\n",
    "      ],\n",
    "      \"palette\": [\n",
    "        \"#66339952\",\n",
    "        \"#ffcc00\"\n",
    "      ],\n",
    "      \"nodata_val\": 0,\n",
    "      \"nodata_color\": \"#ffffff00\"\n",
    "    },\n",
    "  ],\n",
    "  \"deduplicate_at\": [\n",
    "    \"raster\",\n",
    "    \"3dtile\"\n",
    "  ],\n",
    "  \"deduplicate_keep_rules\": [\n",
    "    [\n",
    "      \"Date\",\n",
    "      \"larger\"\n",
    "    ]\n",
    "  ],\n",
    "  \"deduplicate_method\": \"footprints\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stager.stage_all()\n",
    "# took about 26 minutes for all 3 files to stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasterization & web-tiling\n",
    "\n",
    "- Use the `pdgraster.RasterTiler()` function to create the `rasterizer`, then use it to execute the rasterize function: `rasterizer.rasterize_vectors()`.\n",
    "- Alternatively, can run these two steps as one with just `pdgraster.RasterTiler().rasterize_all()`. Just as in staging, you can input the config directly to `RasterTiler()`, or save the config as an object earlier in the script, or save the config as a separate script and source it in. \n",
    "- `rasterize_all()` is a wrapper for `rasterize_vectors()`. It pulls all staged filepaths from the staged dir and rasterizes all z-levels . We do _not_ use `rasterize_all()` when rasterizing in parallel with `parsl` or `ray`. It is exclusively used for small datasets that are not run in parallel. We have to create specific `@parsl` or `@ray.remote` wrapper functions around `stage()` and `rasterize_vectors()` if using those packages for staging, rasterizaiton, etc. For an example, see [here](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/8c1997a9d2456bcb79ba1b3ab0f82b3b2b30b141/IN_PROGRESS_VIZ_WORKFLOW.py#L668-L693) for how we rasterize in the ray workflow.\n",
    "- Rasterization when executed with `rasterize_all()` creates `.tif` files in the output `geotiff` dir, _and_ creates the same number of `.png` files in the output `web_tiles` dir.\n",
    "    - When we use `rasterize_vectors()` instead, we _only create the `.tif` files and not the `.png` files_. So that needs to be executed as a separate step with `rasterizer.webtiles_from_geotiffs()` after the `rasterize_vectors() step`, and _in between those steps_ we need to manually \"update the ranges\" in the rasterizer to ensure that the colors within one z-level of `.png` files looks appropriate when visualized in the portal. We will cross that bridge as necessay, just as we do [here](https://github.com/PermafrostDiscoveryGateway/viz-workflow/blob/8c1997a9d2456bcb79ba1b3ab0f82b3b2b30b141/IN_PROGRESS_VIZ_WORKFLOW.py#L592) in the `ray` workflow on Delta.\n",
    "- The number of z-level 15 tiles in the `staged` dir should match the number of z-level 15 tiles in the `geotiff` and `web_tiles` dirs. The total number of files in both the `geotiff` and `web_tiles` dirs is a _lot_ more than the number of files in `staged` because `staged` _only contains the highest zoom level_ with no parent z-levels.\n",
    "- The web tiles are what we actually visualize on the PDG portal and local cesium. We create the rasters for summary stats (the data behind the web tiles, stored in bands of the `.tif` files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RasterTiler({\n",
    "  \"deduplicate_clip_to_footprint\": True, \n",
    "  \"dir_input\": \"/home/jcohen/iwp_russia_subset_clipToFP_PR/iwp/\", \n",
    "  \"ext_input\": \".shp\",\n",
    "  \"ext_footprints\": \".shp\",\n",
    "  \"dir_footprints\": \"/home/jcohen/iwp_russia_subset_clipToFP_PR/footprints/\", \n",
    "  \"dir_staged\": \"staged/\",\n",
    "  \"dir_geotiff\": \"geotiff/\", \n",
    "  \"dir_web_tiles\": \"web_tiles/\", \n",
    "  \"filename_staging_summary\": \"staging_summary.csv\",\n",
    "  \"filename_rasterization_events\": \"raster_events.csv\",\n",
    "  \"filename_rasters_summary\": \"raster_summary.csv\",\n",
    "  \"filename_config\": \"config\",\n",
    "  \"simplify_tolerance\": 0.1,\n",
    "  \"tms_id\": \"WGS1984Quad\",\n",
    "  \"z_range\": [\n",
    "    0,\n",
    "    15\n",
    "  ],\n",
    "  \"geometricError\": 57,\n",
    "  \"z_coord\": 0,\n",
    "  \"statistics\": [\n",
    "    {\n",
    "      \"name\": \"iwp_coverage\",\n",
    "      \"weight_by\": \"area\",\n",
    "      \"property\": \"area_per_pixel_area\",\n",
    "      \"aggregation_method\": \"sum\",\n",
    "      \"resampling_method\": \"average\",\n",
    "      \"val_range\": [\n",
    "        0,\n",
    "        1\n",
    "      ],\n",
    "      \"palette\": [\n",
    "        \"#66339952\",\n",
    "        \"#ffcc00\"\n",
    "      ],\n",
    "      \"nodata_val\": 0,\n",
    "      \"nodata_color\": \"#ffffff00\"\n",
    "    },\n",
    "  ],\n",
    "  \"deduplicate_at\": [\n",
    "    \"raster\"\n",
    "  ],\n",
    "  \"deduplicate_keep_rules\": [\n",
    "    [\n",
    "      \"Date\",\n",
    "      \"larger\"\n",
    "    ]\n",
    "  ],\n",
    "  \"deduplicate_method\": \"footprints\"\n",
    "}).rasterize_all()\n",
    "\n",
    "# took about 61 min for all staged files to rasterize and web tile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of `geotiff` files produced (for all z-levels): **4376**\n",
    "\n",
    "Number of `web_tiles` produced (for all z-levels): **4376**\n",
    "\n",
    "-----------\n",
    "\n",
    "You can use `rasterio` to plot the rasters if you want to get an idea what the statistics look like. Change the number to choose which band (statistic) to visualize. Example:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "\n",
    "file1 = rasterio.open('/path/to/raster.tif')\n",
    "\n",
    "plt.imshow(file1.read(1), cmap='pink')\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the web tiles\n",
    "\n",
    "Ypu can simply open the `.png` files to view them one at a time, but it's much better to view them on a Cesium basemap! For steps for how to visualize the web tiles with local Cesium, see [documentation here in pdg-info](https://github.com/robyngit/pdg-info/blob/main/05_displaying-the-tiles.md#option-1-run-cesium-locally)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D tiling\n",
    "\n",
    "This step pulls in vector data from the `staged` dir and processes it into 2 file formats: `.b3dm` files and `.json` files that accompany the `.b3dm` files only if they are the highest z-level. This uses the same config as staging and rasterization.\n",
    "\n",
    "When the workflow meets the following criteria:\n",
    "1. It is intended to run through all steps staging, rasterization/web tiling, and 3dtiling\n",
    "2. It is set to deduplicate in the config (rather than setting to `None`),\n",
    "3.  The staged data is intended to be archived and available for others to download and use\n",
    "\n",
    "...then the config is usually set to deduplicate at both `raster` and `3dtiles` rather than `staging`. This is because we want the \"raw\" staging data to be un-deduplicated and available for use when it is archived. See [here](https://github.com/PermafrostDiscoveryGateway/viz-staging/blob/4f31e951600d54c128f76b48a47ec390261fb548/pdgstaging/ConfigManager.py#L231-L294) in the `ConfigManager.py` for this documentation. For data files that do not have accompnaying \"footprints\" like the ice wedge polygon data, the deduplication method would be \"neighbor\". This is the case for DRP data.\n",
    "\n",
    "For 3d-tiling, we need to \"manually\" import `StagedTo3DConverter` (rather than sourcing it in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StagedTo3DConverter():\n",
    "    \"\"\"\n",
    "        Processes staged vector data into Cesium 3D tiles according to the\n",
    "        settings in a config file or dict. This class acts as the orchestrator\n",
    "        of the other viz-3dtiles classes, and coordinates the sending and\n",
    "        receiving of information between them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the StagedTo3DConverter class.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "\n",
    "            config : dict or str\n",
    "                A dictionary of configuration settings or a path to a config\n",
    "                JSON file. (See help(pdgstaging.ConfigManager))\n",
    "        \"\"\"\n",
    "\n",
    "        self.config = pdgstaging.ConfigManager(config)\n",
    "        self.tiles = pdgstaging.TilePathManager(\n",
    "            **self.config.get_path_manager_config())\n",
    "\n",
    "    def all_staged_to_3dtiles(\n",
    "        self\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Process all staged vector tiles into 3D tiles.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the list of staged vector tiles\n",
    "        paths = self.tiles.get_filenames_from_dir('staged')\n",
    "        # Process each tile\n",
    "        for path in paths:\n",
    "            self.staged_to_3dtile(path)\n",
    "\n",
    "    def staged_to_3dtile(self, path):\n",
    "        \"\"\"\n",
    "            Convert a staged vector tile into a B3DM tile file and a matching\n",
    "            JSON tileset file.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            path : str\n",
    "                The path to the staged vector tile.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            tile, tileset : Cesium3DTile, Tileset\n",
    "                The Cesium3DTiles and Cesium3DTileset objects\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Get information about the tile from the path\n",
    "            tile = self.tiles.tile_from_path(path)\n",
    "            out_path = self.tiles.path_from_tile(tile, '3dtiles')\n",
    "\n",
    "            tile_bv = self.bounding_region_for_tile(tile)\n",
    "\n",
    "            # Get the filename of the tile WITHOUT the extension\n",
    "            tile_filename = os.path.splitext(os.path.basename(out_path))[0]\n",
    "            # Get the base of the path, without the filename\n",
    "            tile_dir = os.path.dirname(out_path) + os.path.sep\n",
    "\n",
    "            # Log the event\n",
    "            logger.info(\n",
    "                f'Creating 3dtile from {path} for tile {tile} to {out_path}.')\n",
    "\n",
    "            # Read in the staged vector tile\n",
    "            gdf = gpd.read_file(path)\n",
    "\n",
    "            # Check if the gdf is empty\n",
    "            if len(gdf) == 0:\n",
    "                logger.warning(\n",
    "                    f'Vector tile {path} is empty. 3D tile will not be'\n",
    "                    ' created.')\n",
    "                return\n",
    "\n",
    "            # Remove polygons with centroids that are outside the tile boundary\n",
    "            prop_cent_in_tile = self.config.polygon_prop(\n",
    "                'centroid_within_tile')\n",
    "            gdf = gdf[gdf[prop_cent_in_tile]]\n",
    "\n",
    "            # Check if deduplication should be performed\n",
    "            dedup_here = self.config.deduplicate_at('3dtiles')\n",
    "            dedup_method = self.config.get_deduplication_method()\n",
    "\n",
    "            # Deduplicate if required\n",
    "            if dedup_here and (dedup_method is not None):\n",
    "                dedup_config = self.config.get_deduplication_config(gdf)\n",
    "                dedup = dedup_method(gdf, **dedup_config)\n",
    "                gdf = dedup['keep']\n",
    "\n",
    "                # The tile could theoretically be empty after deduplication\n",
    "                if len(gdf) == 0:\n",
    "                    logger.warning(\n",
    "                        f'Vector tile {path} is empty after deduplication.'\n",
    "                        ' 3D Tile will not be created.')\n",
    "                    return\n",
    "\n",
    "            # Create & save the b3dm file\n",
    "            ces_tile, ces_tileset = TreeGenerator.leaf_tile_from_gdf(\n",
    "                gdf,\n",
    "                dir=tile_dir,\n",
    "                filename=tile_filename,\n",
    "                z=self.config.get('z_coord'),\n",
    "                geometricError=self.config.get('geometricError'),\n",
    "                tilesetVersion=self.config.get('version'),\n",
    "                boundingVolume=tile_bv\n",
    "            )\n",
    "\n",
    "            return ces_tile, ces_tileset\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error creating 3D Tile from {path}.')\n",
    "            logger.error(e)\n",
    "\n",
    "    def parent_3dtiles_from_children(self, tiles, bv_limit=None):\n",
    "        \"\"\"\n",
    "            Create parent Cesium 3D Tileset json files that point to\n",
    "            of child JSON files in the tile tree hierarchy.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            tiles : list of morecantile.Tile\n",
    "                The list of tiles to create parent tiles for.\n",
    "        \"\"\"\n",
    "\n",
    "        tile_manager = self.tiles\n",
    "        config_manager = self.config\n",
    "\n",
    "        tileset_objs = []\n",
    "\n",
    "        # Make the next level of parent tiles\n",
    "        for parent_tile in tiles:\n",
    "            # Get the path to the parent tile\n",
    "            parent_path = tile_manager.path_from_tile(parent_tile, '3dtiles')\n",
    "            # Get just the base dir without the filename\n",
    "            parent_dir = os.path.dirname(parent_path)\n",
    "            # Get the filename of the parent tile, without the extension\n",
    "            parent_filename = os.path.basename(parent_path)\n",
    "            parent_filename = os.path.splitext(parent_filename)[0]\n",
    "            # Get the children paths for this parent tile\n",
    "            child_paths = tile_manager.get_child_paths(parent_tile, '3dtiles')\n",
    "            # Remove paths that do not exist\n",
    "            child_paths = tile_manager.remove_nonexistent_paths(child_paths)\n",
    "            # Get the parent bounding volume\n",
    "            parent_bv = self.bounding_region_for_tile(\n",
    "                parent_tile, limit_to=bv_limit)\n",
    "            # If the bounding region is outside t\n",
    "            # Get the version\n",
    "            version = config_manager.get('version')\n",
    "            # Get the geometric error\n",
    "            geometric_error = config_manager.get('geometricError')\n",
    "            # Create the parent tile\n",
    "            tileset_obj = TreeGenerator.parent_tile_from_children_json(\n",
    "                child_paths,\n",
    "                dir=parent_dir,\n",
    "                filename=parent_filename,\n",
    "                geometricError=geometric_error,\n",
    "                tilesetVersion=version,\n",
    "                boundingVolume=parent_bv\n",
    "            )\n",
    "            tileset_objs.append(tileset_obj)\n",
    "\n",
    "        return tileset_objs\n",
    "\n",
    "    def bounding_region_for_tile(self, tile, limit_to=None):\n",
    "        \"\"\"\n",
    "        For a morecantile.Tile object, return a BoundingVolumeRegion object\n",
    "        that represents the bounding region of the tile.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tile : morecantile.Tile\n",
    "            The tile object.\n",
    "        limit_to : list of float\n",
    "            Optional list of west, south, east, north coordinates to limit\n",
    "            the bounding region to.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bv : BoundingVolumeRegion\n",
    "            The bounding region object.\n",
    "        \"\"\"\n",
    "        tms = self.tiles.tms\n",
    "        bounds = tms.bounds(tile)\n",
    "        bounds = gpd.GeoSeries(\n",
    "            box(bounds.left, bounds.bottom, bounds.right, bounds.top),\n",
    "            crs=tms.crs)\n",
    "        if limit_to is not None:\n",
    "            bounds_limitor = gpd.GeoSeries(\n",
    "                box(limit_to[0], limit_to[1], limit_to[2], limit_to[3]),\n",
    "                crs=tms.crs)\n",
    "            bounds = bounds.intersection(bounds_limitor)\n",
    "        bounds = bounds.to_crs(BoundingVolumeRegion.CESIUM_EPSG)\n",
    "        bounds = bounds.total_bounds\n",
    "\n",
    "        region_bv = {\n",
    "            'west': bounds[0], 'south': bounds[1],\n",
    "            'east': bounds[2], 'north': bounds[3],\n",
    "        }\n",
    "        return region_bv\n",
    "\n",
    "def create_leaf_3dtiles(staged_paths, config, logging_dict=None):\n",
    "    \"\"\"\n",
    "    Create a batch of leaf 3d tiles from staged vector tiles\n",
    "    \"\"\"\n",
    "    from pdg_workflow import StagedTo3DConverter\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    converter3d = StagedTo3DConverter(config)\n",
    "    tilesets = []\n",
    "    for path in staged_paths:\n",
    "        try:\n",
    "            ces_tile, ces_tileset = converter3d.staged_to_3dtile(path)\n",
    "            tilesets.append(ces_tileset)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error creating 3d tile from {path}')\n",
    "            logging.error(e)\n",
    "    return tilesets\n",
    "\n",
    "def create_parent_3dtiles(tiles, config, limit_bv_to=None, logging_dict=None):\n",
    "    \"\"\"\n",
    "    Create a batch of cesium 3d tileset parent files that point to child\n",
    "    tilesets\n",
    "    \"\"\"\n",
    "    from pdg_workflow import StagedTo3DConverter\n",
    "    if logging_dict:\n",
    "        import logging.config\n",
    "        logging.config.dictConfig(logging_dict)\n",
    "    logging.info(f'Creating parent 3d tiles for {len(tiles)} tiles')\n",
    "    converter3d = StagedTo3DConverter(config)\n",
    "    return converter3d.parent_3dtiles_from_children(tiles, limit_bv_to)\n",
    "\n",
    "\n",
    "def make_batch(items, batch_size):\n",
    "    \"\"\"\n",
    "    Create batches of a given size from a list of items.\n",
    "    \"\"\"\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the config as an object for 3dtiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_config = {\n",
    "  \"deduplicate_clip_to_footprint\": True, \n",
    "  \"dir_input\": \"/home/jcohen/iwp_russia_subset_clipToFP_PR/iwp/\", \n",
    "  \"ext_input\": \".shp\",\n",
    "  \"ext_footprints\": \".shp\",\n",
    "  \"dir_footprints\": \"/home/jcohen/iwp_russia_subset_clipToFP_PR/footprints/\", \n",
    "  \"dir_staged\": \"staged/\",\n",
    "  \"dir_geotiff\": \"geotiff/\", \n",
    "  \"dir_web_tiles\": \"web_tiles/\", \n",
    "  \"filename_staging_summary\": \"staging_summary.csv\",\n",
    "  \"filename_rasterization_events\": \"raster_events.csv\",\n",
    "  \"filename_rasters_summary\": \"raster_summary.csv\",\n",
    "  \"filename_config\": \"config\",\n",
    "  \"simplify_tolerance\": 0.1,\n",
    "  \"tms_id\": \"WGS1984Quad\",\n",
    "  \"z_range\": [\n",
    "    0,\n",
    "    15\n",
    "  ],\n",
    "  \"geometricError\": 57,\n",
    "  \"z_coord\": 0,\n",
    "  \"statistics\": [\n",
    "    {\n",
    "      \"name\": \"iwp_coverage\",\n",
    "      \"weight_by\": \"area\",\n",
    "      \"property\": \"area_per_pixel_area\",\n",
    "      \"aggregation_method\": \"sum\",\n",
    "      \"resampling_method\": \"average\",\n",
    "      \"val_range\": [\n",
    "        0,\n",
    "        1\n",
    "      ],\n",
    "      \"palette\": [\n",
    "        \"#66339952\",\n",
    "        \"#ffcc00\"\n",
    "      ],\n",
    "      \"nodata_val\": 0,\n",
    "      \"nodata_color\": \"#ffffff00\"\n",
    "    },\n",
    "  ],\n",
    "  \"deduplicate_at\": [\n",
    "    \"raster\"\n",
    "  ],\n",
    "  \"deduplicate_keep_rules\": [\n",
    "    [\n",
    "      \"Date\",\n",
    "      \"larger\"\n",
    "    ]\n",
    "  ],\n",
    "  \"deduplicate_method\": \"footprints\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch sizes for this step\n",
    "batch_size_3dtiles = 20\n",
    "batch_size_parent_3dtiles = 500\n",
    "\n",
    "stager = pdgstaging.TileStager(workflow_config)\n",
    "tiles3dmaker = StagedTo3DConverter(workflow_config)\n",
    "tile_manager = stager.tiles\n",
    "config_manager = stager.config\n",
    "min_z = config_manager.get_min_z()\n",
    "max_z = config_manager.get_max_z()\n",
    "parent_zs = range(max_z - 1, min_z - 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to all the newly staged tiles\n",
    "staged_paths = stager.tiles.get_filenames_from_dir('staged')\n",
    "staged_batches = make_batch(staged_paths, batch_size_3dtiles)\n",
    "\n",
    "for batch in staged_batches:\n",
    "    create_leaf_3dtiles(batch, workflow_config) # no logging dict setup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipToFP_PR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
